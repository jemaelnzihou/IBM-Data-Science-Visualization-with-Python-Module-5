{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# GRC Big Data Visuals \u2014 Churn, Inventory, Fraud, and Decision Latency\n", "\n", "This notebook contains **four visualizations** that illustrate how a modern big data stack drives business value for **Global Retail Corporation (GRC)**:\n", "\n", "1. **Churn Rate by Risk Decile** \u2014 enables targeted retention campaigns.\n", "2. **Inventory Imbalance Heatmap (DC \u00d7 SKU)** \u2014 guides stock rebalancing to avoid stock-outs/overstock.\n", "3. **Fraud Anomaly Score Over Time** \u2014 surfaces suspicious spikes for review in near-real time.\n", "4. **Decision Latency: Before vs After** \u2014 shows the step-change from batch to streaming analytics.\n", "\n", "> Notes: Uses only `matplotlib` (no seaborn), one chart per figure.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Setup (Deepnote/Jupyter)\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from datetime import datetime, timedelta\n", "np.random.seed(42)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Customer Retention \u2014 Churn Rate by Risk Decile\n", "**Why this matters:** Sorting customers into deciles by predicted churn risk lets GRC run **precision retention** (offers/outreach) where it returns the most value, instead of blanket discounts."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["n_customers = 10000\n", "risk_scores = np.clip(np.random.beta(2, 5, size=n_customers), 0, 1)\n", "churn = (np.random.rand(n_customers) < risk_scores * 0.7).astype(int)\n", "df_churn = pd.DataFrame({\"risk\": risk_scores, \"churn\": churn})\n", "df_churn[\"decile\"] = pd.qcut(df_churn[\"risk\"], 10, labels=False) + 1\n", "churn_by_decile = df_churn.groupby(\"decile\")[\"churn\"].mean().reset_index()\n", "\n", "plt.figure(figsize=(10, 6))\n", "plt.bar(churn_by_decile[\"decile\"], churn_by_decile[\"churn\"] * 100)\n", "plt.title(\"Churn Rate by Risk Decile (Higher = Riskier)\")\n", "plt.xlabel(\"Risk Decile\")\n", "plt.ylabel(\"Observed Churn Rate (%)\")\n", "plt.xticks(range(1, 11))\n", "plt.tight_layout()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Inventory Optimization \u2014 Imbalance Heatmap (DC \u00d7 SKU)\n", "**Why this matters:** Near-real-time DC\u00d7SKU imbalance highlights **where to move stock now**. Negative values imply **stock-out pressure**; positive values imply **overstock** \u2014 powering automated rebalancing."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["n_dc, n_sku = 10, 10\n", "imbalance = np.random.normal(loc=0, scale=1, size=(n_dc, n_sku))\n", "\n", "plt.figure(figsize=(8, 7))\n", "plt.imshow(imbalance, aspect='auto')\n", "plt.title(\"Inventory Imbalance Heatmap (DC x SKU)\")\n", "plt.xlabel(\"SKU Index\")\n", "plt.ylabel(\"Distribution Center Index\")\n", "plt.colorbar(label=\"Imbalance (\u2212 stockout  |  + overstock)\")\n", "plt.tight_layout()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Fraud Detection \u2014 Daily Anomaly Score with Threshold\n", "**Why this matters:** Streaming anomaly detection flags **suspicious periods** for analyst review and automated holds, cutting fraud losses while keeping false positives manageable with a threshold."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["days = 60\n", "dates = [datetime.today().date() - timedelta(days=days - i) for i in range(days)]\n", "baseline = np.random.normal(0.3, 0.05, size=days)\n", "spikes_idx = np.random.choice(range(days), size=5, replace=False)\n", "anomaly_scores = baseline.copy()\n", "anomaly_scores[spikes_idx] += np.random.uniform(0.3, 0.6, size=5)\n", "threshold = 0.5\n", "\n", "plt.figure(figsize=(12, 5))\n", "plt.plot(dates, anomaly_scores, marker='o')\n", "plt.axhline(threshold)\n", "plt.title(\"Daily Fraud Anomaly Score (with Threshold)\")\n", "plt.xlabel(\"Date\")\n", "plt.ylabel(\"Anomaly Score\")\n", "plt.xticks(rotation=45, ha='right')\n", "plt.tight_layout()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Decision Acceleration \u2014 Latency Before vs After Big Data Stack\n", "**Why this matters:** Moving from batch ETL to a streaming stack (Kafka/Kinesis + Spark/Flink) collapses time-to-insight from **hours to minutes**, enabling timely pricing, promo, and ops decisions."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["labels = [\"Batch Legacy\", \"Streaming Stack\"]\n", "latency_minutes = [720, 5]\n", "\n", "plt.figure(figsize=(7, 5))\n", "plt.bar(labels, latency_minutes)\n", "plt.title(\"Decision Latency Before vs After Big Data Stack\")\n", "plt.ylabel(\"Minutes to Insight\")\n", "plt.tight_layout()\n", "plt.show()\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}